{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid: [0.26894142 0.5        0.73105858 0.88079708]\n",
      "Tanh: [-0.76159416  0.          0.76159416  0.96402758]\n",
      "ReLU: [0. 0. 1. 2.]\n",
      "Leaky ReLU: [-0.01  0.    1.    2.  ]\n",
      "Softmax: [0.0320586  0.08714432 0.23688282 0.64391426]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "    return np.where(x > 0, x, x * alpha)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))\n",
    "    return exp_x / exp_x.sum(axis=0)\n",
    "\n",
    "# Example usage:\n",
    "x = np.array([-1.0, 0.0, 1.0, 2.0])\n",
    "print(\"Sigmoid:\", sigmoid(x))\n",
    "print(\"Tanh:\", tanh(x))\n",
    "print(\"ReLU:\", relu(x))\n",
    "print(\"Leaky ReLU:\", leaky_relu(x))\n",
    "print(\"Softmax:\", softmax(x))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

### Mean Squared Error (MSE)

#### Formula
![image](https://github.com/user-attachments/assets/5c9bb781-0e98-4839-9dd6-e1a93437d264)

#### Description
The MSE is a risk metric corresponding to the expected value of the squared (quadratic) error or loss. By squaring the errors before averaging them, MSE gives a relatively high weight to large errors. This means that large deviations from the actual values are penalized more severely than small deviations. The squaring also ensures that error values are positive, as the square of a real number is always non-negative.

The primary reason to use MSE is its differentiability, which allows it to work well with optimization algorithms that calculate gradients, such as gradient descent. The square function has a well-defined derivative at every point, which simplifies the computation of gradients needed for model training.

#### Application
MSE is widely used in regression tasks, where the goal is to predict continuous or quantitative outputs. It is one of the most common loss functions used in various types of regression models, including linear regression, polynomial regression, and many types of neural networks that are used for predictive tasks.

#### Example: Predicting House Prices
Consider a regression model designed to predict house prices based on features such as size (in square feet), location (city or suburbs), and number of rooms. Here’s how MSE comes into play:

1. **Model Prediction**: For each house in the training dataset, the model predicts a price based on its features.
2. **Error Calculation**: For each house, calculate the error, which is the difference between the actual price and the predicted price.
3. **Error Squaring**: Square each error to eliminate negative values and give more weight to larger errors.
4. **Average Calculation**: Compute the average of these squared errors across all houses in the dataset. This average is the MSE.

In this example, if the model predicts a price of $300,000 for a house whose actual price is $500,000, the error is $200,000. The squared error is \(200,000^2\), which is quite large, significantly influencing the MSE. This large error value, when squared, becomes even larger, emphasizing the need for the model to reduce these errors significantly during training.

#### Advantages of MSE
- **Interpretability**: Since MSE is simply the average of squared errors, it’s relatively easy to interpret and explain.
- **Sensitivity to Large Errors**: Because errors are squared, MSE is very sensitive to large errors. This property can be particularly useful when large errors are undesirable or particularly costly.

#### Limitations of MSE
- **Impact of Outliers**: The squaring of errors means that outliers (which are significantly different from other data points) can disproportionately affect the MSE. Thus, MSE may not be the best choice if the data is susceptible to many outliers.
- **Scale Dependence**: MSE does not provide a scale-independent metric. Its value depends on the scale of the target variable, making it sometimes difficult to compare MSE across different datasets.

Overall, Mean Squared Error is a robust and reliable measure for many regression tasks, providing clear feedback for model improvement through its emphasis on reducing larger errors more significantly.

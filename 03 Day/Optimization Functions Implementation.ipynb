{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizers:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def gradient_descent(self, params, gradients):\n",
    "        \"\"\"\n",
    "        Standard Gradient Descent\n",
    "        Simple but effective for convex problems\n",
    "        \"\"\"\n",
    "        return params - self.learning_rate * gradients\n",
    "    \n",
    "    def momentum(self, params, gradients, velocity, momentum=0.9):\n",
    "        \"\"\"\n",
    "        Momentum-based Gradient Descent\n",
    "        Helps overcome local minima and speeds up convergence\n",
    "        \"\"\"\n",
    "        velocity = momentum * velocity + self.learning_rate * gradients\n",
    "        return params - velocity, velocity\n",
    "    \n",
    "    def rmsprop(self, params, gradients, cache, decay_rate=0.9, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        RMSprop Optimizer\n",
    "        Adapts learning rates based on recent gradients\n",
    "        \"\"\"\n",
    "        cache = decay_rate * cache + (1 - decay_rate) * np.square(gradients)\n",
    "        update = self.learning_rate * gradients / (np.sqrt(cache) + epsilon)\n",
    "        return params - update, cache\n",
    "    \n",
    "    def adam(self, params, gradients, moment, velocity, t, \n",
    "            beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Adam Optimizer\n",
    "        Combines benefits of momentum and RMSprop\n",
    "        \"\"\"\n",
    "        moment = beta1 * moment + (1 - beta1) * gradients\n",
    "        velocity = beta2 * velocity + (1 - beta2) * np.square(gradients)\n",
    "        \n",
    "        # Bias correction\n",
    "        moment_corrected = moment / (1 - beta1**t)\n",
    "        velocity_corrected = velocity / (1 - beta2**t)\n",
    "        \n",
    "        update = self.learning_rate * moment_corrected / (np.sqrt(velocity_corrected) + epsilon)\n",
    "        return params - update, moment, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple 2D optimization problem (bowl-shaped function)\n",
    "def objective_function(x, y):\n",
    "    return x**2 + y**2\n",
    "\n",
    "def gradient_function(x, y):\n",
    "    return np.array([2*x, 2*y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the optimization problem\n",
    "x = np.linspace(-2, 2, 100)\n",
    "y = np.linspace(-2, 2, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = objective_function(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize starting points and parameters\n",
    "start_points = np.array([-1.5, 1.5])\n",
    "learning_rate = 0.1\n",
    "n_iterations = 50\n",
    "\n",
    "# Initialize optimizers\n",
    "optimizer = Optimizers(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track paths for different optimizers\n",
    "paths = {\n",
    "    'GD': [start_points.copy()],\n",
    "    'Momentum': [start_points.copy()],\n",
    "    'RMSprop': [start_points.copy()],\n",
    "    'Adam': [start_points.copy()]\n",
    "}\n",
    "\n",
    "# Optimization loop\n",
    "current_points = {\n",
    "    'GD': start_points.copy(),\n",
    "    'Momentum': start_points.copy(),\n",
    "    'RMSprop': start_points.copy(),\n",
    "    'Adam': start_points.copy()\n",
    "}\n",
    "\n",
    "# Initialize optimizer states\n",
    "velocity = np.zeros_like(start_points)  # For Momentum\n",
    "cache = np.zeros_like(start_points)     # For RMSprop\n",
    "moment = np.zeros_like(start_points)    # For Adam\n",
    "velocity_adam = np.zeros_like(start_points)  # For Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization\n",
    "for t in range(n_iterations):\n",
    "    # Gradient Descent\n",
    "    grad = gradient_function(*current_points['GD'])\n",
    "    current_points['GD'] = optimizer.gradient_descent(current_points['GD'], grad)\n",
    "    paths['GD'].append(current_points['GD'].copy())\n",
    "    \n",
    "    # Momentum\n",
    "    grad = gradient_function(*current_points['Momentum'])\n",
    "    current_points['Momentum'], velocity = optimizer.momentum(\n",
    "        current_points['Momentum'], grad, velocity)\n",
    "    paths['Momentum'].append(current_points['Momentum'].copy())\n",
    "    \n",
    "    # RMSprop\n",
    "    grad = gradient_function(*current_points['RMSprop'])\n",
    "    current_points['RMSprop'], cache = optimizer.rmsprop(\n",
    "        current_points['RMSprop'], grad, cache)\n",
    "    paths['RMSprop'].append(current_points['RMSprop'].copy())\n",
    "    \n",
    "    # Adam\n",
    "    grad = gradient_function(*current_points['Adam'])\n",
    "    current_points['Adam'], moment, velocity_adam = optimizer.adam(\n",
    "        current_points['Adam'], grad, moment, velocity_adam, t+1)\n",
    "    paths['Adam'].append(current_points['Adam'].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final positions and objective values:\n",
      "--------------------------------------------------\n",
      "GD:\n",
      "  Final position: (-0.0000, 0.0000)\n",
      "  Final value: 0.0000\n",
      "\n",
      "Momentum:\n",
      "  Final position: (0.0457, -0.0457)\n",
      "  Final value: 0.0042\n",
      "\n",
      "RMSprop:\n",
      "  Final position: (0.0000, -0.0000)\n",
      "  Final value: 0.0000\n",
      "\n",
      "Adam:\n",
      "  Final position: (-0.0764, 0.0764)\n",
      "  Final value: 0.0117\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print final results\n",
    "print(\"\\nFinal positions and objective values:\")\n",
    "print(\"-\" * 50)\n",
    "for method, point in current_points.items():\n",
    "    final_value = objective_function(*point)\n",
    "    print(f\"{method}:\")\n",
    "    print(f\"  Final position: ({point[0]:.4f}, {point[1]:.4f})\")\n",
    "    print(f\"  Final value: {final_value:.4f}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

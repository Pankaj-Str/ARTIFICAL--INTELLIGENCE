{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizers:\n",
    "    def __init__(self, learning_rate=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def gradient_descent(self, params, gradients):\n",
    "        \"\"\"\n",
    "        Standard Gradient Descent\n",
    "        Simple but effective for convex problems\n",
    "        \"\"\"\n",
    "        return params - self.learning_rate * gradients\n",
    "    \n",
    "    def momentum(self, params, gradients, velocity, momentum=0.9):\n",
    "        \"\"\"\n",
    "        Momentum-based Gradient Descent\n",
    "        Helps overcome local minima and speeds up convergence\n",
    "        \"\"\"\n",
    "        velocity = momentum * velocity + self.learning_rate * gradients\n",
    "        return params - velocity, velocity\n",
    "    \n",
    "    def rmsprop(self, params, gradients, cache, decay_rate=0.9, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        RMSprop Optimizer\n",
    "        Adapts learning rates based on recent gradients\n",
    "        \"\"\"\n",
    "        cache = decay_rate * cache + (1 - decay_rate) * np.square(gradients)\n",
    "        update = self.learning_rate * gradients / (np.sqrt(cache) + epsilon)\n",
    "        return params - update, cache\n",
    "    \n",
    "    def adam(self, params, gradients, moment, velocity, t, \n",
    "            beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Adam Optimizer\n",
    "        Combines benefits of momentum and RMSprop\n",
    "        \"\"\"\n",
    "        moment = beta1 * moment + (1 - beta1) * gradients\n",
    "        velocity = beta2 * velocity + (1 - beta2) * np.square(gradients)\n",
    "        \n",
    "        # Bias correction\n",
    "        moment_corrected = moment / (1 - beta1**t)\n",
    "        velocity_corrected = velocity / (1 - beta2**t)\n",
    "        \n",
    "        update = self.learning_rate * moment_corrected / (np.sqrt(velocity_corrected) + epsilon)\n",
    "        return params - update, moment, velocity"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
